# -*- coding: utf-8 -*-
"""Analise de sentimentos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UHC8SSTxyL78MqSCmEg3VFLX2afPN9kR
"""

import numpy as np
import pandas as pd
import nltk
import gensim
nltk.download('punkt')
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

import torch
import torch.nn as nn
from torch import optim

data = pd.read_json("reviews.json")
data.text = data.text.astype(str)
data

id2c = {0: 'neg', 1:'neutral', 2: 'pos'}
data['sentiment'].unique()

size = int(len(data) * 0.2)
treino = data[size:]
teste = data[:size]

def get_features(data):
  y = 0 if data['sentiment'] == 'neg' else 1 if data['sentiment'] == 'neutral' else 2

  return {'X': data['text'], 'y': y}

first = data.iloc[33004]

get_features(first)

def tokenize(texto):
  return nltk.word_tokenize(texto)

pipe = Pipeline([
  ('count', CountVectorizer(tokenizer=tokenize, stop_words=stopwords, min_df=5)),
  ('tfidf', TfidfTransformer()),
])

feat_treino = [get_features(treino.iloc[w]) for w in range(len(treino))]
treino_X = [w['X'] for w in feat_treino]
treino_y = [w['y'] for w in feat_treino]

pipe.fit(treino_X, treino_y)

nvocab = pipe.transform(["Test"]).shape[1]
nvocab

class SentimentPredictor(nn.Module):
  def __init__(self,inp_dim, hdim, n_classes, tfidfer):
    super(SentimentPredictor, self).__init__()
    self.tfidfer = tfidfer
    self.layer1 = nn.Linear(inp_dim, hdim)
    self.activation = nn.Sigmoid()
    self.layer2 = nn.Linear(hdim, n_classes)
    self.softmax = nn.LogSoftmax(1)

  def forward(self, X):
    tfidf = self.tfidfer.transform(X).toarray()
    tfidf = torch.tensor(tfidf).float()

    l1 = self.layer1(tfidf)
    ac = self.activation(l1)
    l2 = self.layer2(ac)

    return self.softmax(l2)

inp_dim = nvocab
hdim = 1024
n_classes = len(set(treino_y))
model = SentimentPredictor(inp_dim, hdim, n_classes, pipe)

nepochs = 3
batch_size = 256
batch_status = 77
learning_rate = 0.001
criterion = nn.NLLLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

from torch.utils.data import DataLoader, Dataset

traindata = DataLoader([get_features(treino.iloc[w]) for w in range(len(treino))], batch_size=batch_size, shuffle=True)
testdata = DataLoader([get_features(treino.iloc[w]) for w in range(len(teste))], batch_size=batch_size, shuffle=True)

from sklearn.metrics import f1_score

def evaluate(testdata):
  y_real, y_pred = [], []
  for batch_idx, row in enumerate(testdata):
    outputs = model(row['X'])
    y_pred.extend(torch.argmax(outputs, 1).tolist())

    y_real.extend(row['y'])

  print('F1-Score:', f1_score(y_real, y_pred, average='weighted'))

for epoch in range(nepochs):
  losses = []
  for batch_idx, row in enumerate(traindata):
    X = row['X']
    y = row['y']

    # Forward
    outputs = model(X)

    # Calculate loss
    loss = criterion(outputs, y)
    losses.append(float(loss))

    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Display
    if (batch_idx+1) % batch_status == 0:
      print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tTotal Loss: {:.6f}'.format(
                            epoch+1, batch_idx + 1, len(traindata),
                            100. * batch_idx / len(traindata), float(loss),
                            round(sum(losses) / len(losses), 5)))

  evaluate(testdata)

text =  'Disse em flashback o filme abre em <digit><digit><digit><digit> com Charlie sendo premiado por seu papel na derrota do comunismo. Devo admitir que meu coração afundou como no pensamento de ter que suportar mais uma história de vida séria um tanto chata e excessivamente longa. Quão errado eu estava porque essa curta cena é o mais perto que o filme chega a ficar entediante. O filme é cheio de cenários divertidos e divertidos e de diálogos violentos em alguns dos lugares mais inesperados. A próxima cena após a cerimônia de premiação é Charlie em um Hot-Tub com algumas mulheres nuas e um cara tentando levá-lo a investir em um programa de TV. Outra cena bastante divertida é de cerca de <digit> quartos no filme compreende Charlie um grupo de seus secretários bastante sexy Phillip Seymour Hoffmans CIA Man e uma garrafa de uísque. Quanto ao diálogo o que dizer de uma linha "O Senador diz: Ele pode nos ensinar a digitar mas não pode nos ensinar a cultivar Tits". OK o menino da escola eu sei mas o filme é atado com grandes falas.As performances bem Phillip Seymour Hoffman como de costume rouba todas as cenas em que ele está. Hanks está OK mas surpreendentemente para mim de qualquer maneira foi Julia Roberts que é muito boa no papel de um pouco excêntrico Texas Oil Millionairess.Charlie Wilsons War é um dos melhores não-músicos Bio-pics em um longo tempo além de ser aquela coisa rara um filme que diverte diverte bem como informa todos na mesma medida.'

opt = model([text])
result = torch.argmax(opt,1).tolist()
result